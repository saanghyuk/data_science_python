# Deep Learning 3

- #### 신경망과 다양한 데이터

   이번에는 이미지가 아니라, 표 데이터를 다룰때 신경망을 어떻게 쓰는지 보게 될 것. 신경망에서는 MNIST데이터든, 표 데이터든 바뀌는 것은 아무것도 없음. ![3_1](./resources/3_1.png)

  지금까지 했던것은, 이미지 데이터를 픽셀로 표현해서 표 데이터로 만든 다음에 신경망에 돌린 것. 표 데이터는 이미 표로 정리되어 있기 때문에, 그냥 사용하면 되는 것.

  예를 들어 여러 환자에 대한 데이터가 있다고 해보자. 입력변수는 환자에 대한 다양한 수치들. 해당 환자가 독감에 걸렸는지 안걸렸는지가 출력변수. 

  ![3_1](./resources/3_2.png)

  그럼 그냥 신경망의 입력층에, 각 층에 환자의 feature들이 하나씩 들어가겠지. 784개의 뉴런 대신, 4개의 뉴런만 사용하면 되겠지. 그리고, 숫자 10개중 하나로 분류하는게 아니라, 독감 환자인지 아닌지를 분류하는 것. 출력층에서는 뉴런을 1개만 쓰면 되겠지. **일반적으로 은닉층의 뉴런갯수는 입력층과 출력층 사이의 갯수로 정하게 됨.** 

  ![3_1](./resources/3_3.png)

  이제 나머지는 다 똑같아. 뉴런의 갯수만 생각하면 나머지는 다 똑같음. 



- #### 신경망의 비선형성

  선형적인 머신러닝에 대해서 봐보자. 

  대표적인 선형적인 알고리즘인 로지스틱회귀에 대해서 봐보자. 약 투여에 따른 효과 있고, 없었던 상황에 대해서 생각해보자. 로지스틱회귀의 목적은 데이터에 가장 잘 맞는, 시그모이드 함수를 찾는 것이였음. 이 함수에 입력변수를 넣었을 때, 결과값이 0.5가 넘으면 효과가 있는 것이였고, 0.5보다 낮으면 효과가 없는 것으로 예측하는 알고리즘 이였음. 

  ![3_1](./resources/3_4.png)

  변수 하나일때, 여기서 실질적으로 데이터를 분류하는 기준은 0.5인 점의 x좌표 찾으면 10을 넘었냐 안넘었냐였음. 데이터를 두개로 나누는 결정선을 바로 **결정경계(Decision Boundary)**라고 했던 것. 

  ![3_1](./resources/3_5.png)

  만약 feature가 두개라면 어떻게 될까? 

  ![3_1](./resources/3_6.png)

  **어쨋든 로지스틱 회귀를 사용하면, 선형적인 결정경계만 얻을 수 있음.** 

  만약에 데이터가 아래처럼 있으면 어떻게 될까? 데이터를 잘 분리하는 기준을 찾는 것이 어려워짐. 이런 데이터를 보고 **선형분리불가능**하다고 말함. 영어로 linearly inseparable. 

  ![3_1](./resources/3_7.png)

  ![3_1](./resources/3_8.png)

  **그러니깐 이런 경우는 로지스틱 회귀라는 알고리즘 자체가 안먹히는 거지.** 

  이럴때 신경망을 사용하면, 훨씬 더 복잡한 결정경계를 찾아낼 수 있음. 

  ![3_1](./resources/3_9.png)

  ![3_1](./resources/3_10.png)

  위같은 경우는 매우 간단한 결정경계에 속함. 훨씬 더 복잡한 결정경계도 찾아낼 수 있음. 

  ![3_1](./resources/3_11.png)

  **신경망의 이런 비선형성은 은닉층의 활성함수를 통해 이뤄진다.** *활성함수는 전 층 뉴런들의 출력과 가중치를 곱하고 편향을 더한 후 시그모이드에 넣는 것*. 

  ![3_1](./resources/3_12.png)

  **지금까지는 시그모이드를 활성함수로 사용했는데, 당연히 꼭 시그모이드를 사용해야 하는 것은 아니겠지.** 이때 은닉층의 활성함수가 주로 **비선형함수**를 사용함. 비선형함수는 그냥 선형함수가 아니라는 말. 

  ![3_1](./resources/3_13.png)

  활성함수로 사용했던 시그모이드도 비선형 함수지. 

  신경망도 결국 인풋을 받아서, 아웃풋을 계산하는 하나의 함수. 입력층으로 들어간 데이터는 각 층을 넘어갈때마다 활성함수를 넘어가지. 이 활성함수가 비선형이면, **결국 신경망 이라는 함수도 엄청나게 복잡한 비선형 합성함수가 되는 것.** 

  시그모이드를 예로 들면, 시그모이드로 처리된게 또 시그모이드를 넘어가고 이렇게 층마다 반복되는 것. 그리고, 이 비선형함수로 결정경계를 만들면 비선형적이게 나옴. 

  반면, 활성화함수가 선형이면, 신경망도 결국 선형함수가 나오고, 결정경계도 선형함수가 나옴. 



- #### 로지스틱회귀가 선형 모델(선형의 결정경계를 갖는 모델)인 이유

  ![3_1](./resources/3_14.png)

  ![3_1](./resources/3_15.png)

