#  기본 지도 학습 알고리즘

## Linear Regression 

- 집 가격 예측 알고리즘을 만들고자 함. 문제를 단순화 하기 위해서, 오직 집값을 가지고 만들고자 함. 데이터를 그래프에 찍으면 아래와 같음. 이 집 데이터를 프로그램에게 학습시키고자 함. 그 후에 이 프로그램이 어떤 새로운 데이터(집 크기)를 받았을 때 가격을 예측하도록 만들고자 함. 

  ![2_1](./resources/2_1.png)

- **선형회귀는 여기 있는 데이터를 가장 잘 대변해 주는 선을 찾는 것**. 이 선을 **최적선(line of best fit)**이라고 함.  이 선에서 이제 그냥 크기를 넣으면 가격이 예측 되는 것. 꽤 합리적인 예측이 가능함. 

  ![2_1](./resources/2_2.png)

- #### 선형회귀 용어

  머신러닝은 **지도학습**, **비지도학습, 강화학습** 세가지로 나뉨. 이 셋중에 선형회귀는 **지도학습** 알고리즘(답이 존재하니깐). 그리고 지도학습은 분류와 회귀로 나뉘게 됨. 이 중에, **회귀**. 연속적인 어떤 값 중에 마추는 거니깐. 만약 이 집이 오피스텔인지, 아파트인지, 주택인지 알아내야 하는 것이라면 그것은 분류겠지. 

  ![2_1](./resources/2_3.png)

- #### 데이터 표현법. 

  프로그램 학습시키기 위한 데이터는 **학습데이터.** 학습데이터의 갯수를 **m**이라는 문자로 표현함. **입력변수는 x, 목표변수는 y라는 문자로 표현함.** 

  데이터가 50개 있으면, 이것은 각각 어떻게 표현할까? 그냥 features는 x위에 괄호 1, 괄호 2 이렇게 쓰는 것. 목표변수는 y위에 괄호1, 괄호 2 쓰면 됨. 

  ![2_1](./resources/2_4.png)

- #### 가설함수

  선형회귀에서 하려는 것은 최적선을 찾고자 하는 것. 최적선을 찾아내기 위해 다양한 함수를 시도 해봐야 함. 시도하는 이 함수 하나하나를 가설함수(Hypothesis Function)이라고 부름. 결국 (변수가 하나일 때) **선형회귀**의 의무는 **계수 a와 상수 b**를 찾아내는 것. 

  ![2_1](./resources/2_5.png)

  일반적으로 가설함수를 쓸때는 a, b를 쓰지 않고 h(x)와 세타를 씀. 왜 이렇게 쓸까? 이번 함수에서는 입력변수가 한개니깐 그냥 a, b 이렇게 되니깐 하다 보면 이 변수가 보통은 몇십개 몇백개씩 됨. 그러면 다 어떻게 쓸꺼야? 그런 경우 일관성있게 쓰기 위해 아래와 같이 사용하는 것. **선형회귀의 임무는 결국 이 세타 값들을 찾아내는 것**.  

  ![2_1](./resources/2_6.png)

  

- #### 평균제곱오차(MSE)

  입력변수 하나인 상황을 가정 중. 세개의 선 아무거나 그어봤음. 어떤게 가장 적합할까? 뭔가 두번째가 잘 맞는것 같은데 기준이 뭐야? 평가하는 방법이 뭐야. 그게 바로 **평균제곱오차 : Mean Squared Error**임. 이 데이터들과 가설함수가 평균적으로 얼마나 떨어져있는지를 나타내는 방식. 

  ![2_7](./resources/2_7.png)

  각 데이터의 값고, 가설함수가 예측한 값이 조금씩 차이가 남. 지금 하단의 보라색 점을 가설함수에 넣어보면, 집 가격은 18.8억으로 예측되지만, 실제로는 22억으로 예측됨. **차이는 -3.2억.** 이런 식으로 오차를 다 구할 수 있음.  이 오차들을 싹다 제곱하고 제곱한 값들을 더함. 그리고 총 데이터 갯수만큼 나눔. 이게 평균제곱오차. 이게 가장 작은 놈을 구하자는 것. 

  굳이 왜 제곱을 해서 더할까? 당연하지. 양수일때도 있고, 음수로 올때도 있는데, 차이는 엄청 많이 나는데 그게 서로 다 상쇄되서 차이가 0되면? 그걸 방지하기 위해서 ***양수와 음수를 '차이'라는 기준으로 똑같이 취급***하는거야. 절대값처럼. 그리고 두번째 이유는 ***오차가 커질수록 더 부각시키기 위해서***. 더 큰 오차에 대해서는 더 큰 패널티를 주기 위함. 

  ![2_7](./resources/2_8.png)

  이제 처음 계산하려고 했던 것의 평균제곱오차를 찾아보면 아래와 같음. 이런 기준으로 구할 수 있다는 것. 가장 작은 것을 픽. 

  ![2_7](./resources/2_9.png)



-  **이제 일반화 해보자.**

  가설함수에 x를 넣은 것을 실제 값에서 뺀 후 제곱. 그걸 싹다 m개 시그마 해서 더하고, 전체 갯수로 나눔. 		 			![2_7](./resources/2_10.png)



- #### 손실함수(Loss Function)  = 비용함수(Cost Function)

  선형회귀는 단순히 그냥 데이터에 가장 잘 맞는 선을 찾아내는 것. 이 선을 line of best fit이라고 하고. 이 선을 찾기 위해서는 다양하게 시도를 해 보면서 가장 잘 맞는 것을 골라야 함. 그 중 한가지 방법으로 MSE가 가장 작은 값들을 찾아내는 과정이였음. 그 시도해보는 하나하나의 선이 **가설함수**였음. 

  **평균제곱오차가 크다는 것은 가설함수가 데이터에 잘 안 맞는다는 것이고 평균제곱오차가 작다는 것은 가설함수가 데이터에 잘 맞다는 것.** 

  

  손실함수는 가설함수의 성능을 평가하는 함수. 손실함수의 아웃풋이 작을 수록 가설함수가 데이터에 잘 맞다는 것. 

  손실함수는 보통 J라는 문자를 쓰며, 선형회귀에서는 보통 평균제곱오차를 손실함수로 사용함. m대신 2m을 쓴것은 계산을 편하게 하기 위한 장치임. 어차피 2로 나누면, 순위 결과는 변함이 없음. 

  ![2_11](./resources/2_11.png)

  여기서 중요한 것은 **손실함수 J의 인풋은 세타가 됨.** 당연히 세타일수 밖에 없지. 각 세타들일때, 마다 MSE라는 손실함수를 구하는 거니깐. 애초에 여기서 지금 세타 말고 아무런 변수가 없잖아. **i**는 다 대입되는 값이고.  





- #### 경사하강법(Gradient Descent)

  결국 세타를 찾아가는 과정이 선형회귀를 풀어가는 과정. 그 기준으로서 손실함수를 쓰고 있었던 것. 그 손실함수를 세타 값들을 바꿔가면서, 가장 좋은 선형회귀선을 찾는게 우리가 하고 있는 과정. 

  **손실함수는 변수가 세타라는 것을 잊지 말자. 우리는 적절한 세타를 찾아가는 과정임.** 이제 그럼 세타라는 변수를 가진 J라는 함수가 있겠지. 

  단지 손실함수가 아래와 같다고 가정을 한다면(실제 1변수 1개짜리로 해보면 2차식이 나옴), 가장 작은 극소값을 찾고 싶다는 것. 랜덤한 세타값으로 가설함수를 정했다고 해보자. 현재 지점이 기울기가 -2라면, 오른쪽 방향으로 가면 되겠지. 오른쪽으로 움직여 가는 것. 이렇게 극소법을 찾아가는 것이 극소점을 향해 내려가는 것. 

  ![2_12](./resources/2_12.png)

  근데 위 식은 지금 세타를 하나만 가정했음. 손실함수는 변수 2개에 대한 함수인데, 변수 한개에 대한 함수만을 그린 거야. 

  근데 변수 2개로 해도 똑같아. 여기서 어떻게 내려갈래? 보라색 점에서의 기울기는, 각 인풋변수에 대해서 편미분을 하면 됨. 편미분한 결과식에 현재 위치의 좌표값을 대입하고, 이걸 벡터로 만들면 이게 바로 이 위치에서의 내려가야 하는 기울기. J라는 함수를 편미분한 벡터를 만들고, 그 벡터에 이 각각의 좌표값을 대입하면 그 방향으로 가는게 가장 빠르게 내려가는 방향이라는 것(-를 붙이면). 살짝 내려가고 다시 그 지점에서의 내려가는 벡터를 구해서 그 방향으로 가고 이걸 반복하는 게 그냥 Gradient Descent임. 

  ![2_12](./resources/2_14.png)

  

  ![2_12](./resources/2_13.jpeg)



- #### 경사하강법 테크닉

  계속해서 아래로 내려가면 됨. 현재 Gradient Function(편미분 벡터)가 나와있고, 현재 세타0가 -3이고, 세타 1도 -3이라면 이제 어떻게 내려가면 될까? 

  가장 가파르게 내려가는 방향은 알았어. 

  이제 세타0과 세타1에 각각 그 움직이는 방향으로 움직이면 끝이야. 

  새로운 세타0는 **-3 - 학습률a * (-6)**  -> 학습률 알파를 넣는 이유는 얼마나 많이/적게 움직일지에 대해서 말하기 위함. 학습률을 0.1이라고 하면 식은 아래와 같음. 세타1도 똑같아.

  ![2_12](./resources/2_15.png)

  경사하강 한번 하니깐, 세타0과 세타1의 값이 바뀜. 근데 여기서 주의해야 되는 문제가, 세타0을 업데이트 한 다음에 세타1을 업데이트 하려고 하면, 세타 1을 구하면서, **J에 업데이트된 세타0을 넣을 수가 있음**. **반드시 바뀌기 전 값을 넣어야 한다는 것을 기억하자**. 

  ![2_12](./resources/2_16.png)

  이걸 반복하다 보면, 손실함수의 극소점에 가까이 갈 수 있고 최적의 세타 값들을 찾을 수 있게 되는 것. 

  ![2_17](./resources/2_17.png)

  ![2_17](./resources/2_18.png)

  <img src="./resources/2_19.png" alt="2_17" style="zoom:120%;" />	![2_17](./resources/2_20.png)



- 선형회귀 구현하기 쉽게 표현하기

  ![2_21](./resources/2_21.png)

  ![2_21](./resources/2_22.png)

  ![2_21](./resources/2_23.png)

  ![2_21](./resources/2_24.png)

  ![2_21](./resources/2_2	5.png)